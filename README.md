# Faster-Whisper API

Uma API REST para transcriÃ§Ã£o de Ã¡udio usando [faster-whisper](https://github.com/SYSTRAN/faster-whisper), otimizada para CPU com suporte a Docker.

## ğŸš€ CaracterÃ­sticas

- âš¡ **RÃ¡pido**: AtÃ© 4x mais rÃ¡pido que o Whisper original
- ğŸ–¥ï¸ **Otimizado para CPU**: Configurado com quantizaÃ§Ã£o INT8
- ğŸ”Š **MÃºltiplos formatos**: MP3, WAV, M4A, OGG, FLAC, AAC
- ğŸ“ **Word timestamps**: Suporte a timestamps de palavras
- ğŸ¯ **VAD Filter**: Filtro de detecÃ§Ã£o de atividade vocal
- ğŸ³ **Docker**: Deploy fÃ¡cil com Docker e Docker Compose
- ğŸ“š **DocumentaÃ§Ã£o automÃ¡tica**: Swagger UI integrado

## ğŸ“‹ PrÃ©-requisitos

- Python 3.11+
- Docker (opcional, mas recomendado)
- FFmpeg (para processamento de Ã¡udio)

## ğŸ› ï¸ InstalaÃ§Ã£o

### Usando Docker (Recomendado)

1. **Clone/baixe os arquivos**:
```bash
git clone <seu-repositorio>
cd whisper-api
```

2. **Execute com Docker Compose**:
```bash
docker-compose up --build
```

3. **Acesse a API**:
   - API: http://localhost:8000
   - DocumentaÃ§Ã£o: http://localhost:8000/docs

### InstalaÃ§Ã£o Local

1. **Instale as dependÃªncias**:
```bash
pip install -r requirements.txt
```

2. **Execute a aplicaÃ§Ã£o**:
```bash
python main.py
```

## ğŸ“– Uso da API

### Endpoints DisponÃ­veis

- `GET /` - Status da API
- `GET /health` - Health check
- `POST /transcribe` - Transcrever um arquivo de Ã¡udio
- `POST /transcribe-batch` - Transcrever mÃºltiplos arquivos

### Exemplos de Uso

#### 1. TranscriÃ§Ã£o Simples

```bash
curl -X POST "http://localhost:8000/transcribe" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@audio.mp3"
```

#### 2. Com ParÃ¢metros Personalizados

```bash
curl -X POST "http://localhost:8000/transcribe" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@audio.wav" \
     -F "language=pt" \
     -F "word_timestamps=true" \
     -F "beam_size=10"
```

#### 3. Usando Python

```python
import requests

# TranscriÃ§Ã£o simples
with open("audio.mp3", "rb") as audio_file:
    response = requests.post(
        "http://localhost:8000/transcribe",
        files={"file": audio_file}
    )
    result = response.json()
    print(result["text"])

# Com parÃ¢metros personalizados
with open("audio.wav", "rb") as audio_file:
    response = requests.post(
        "http://localhost:8000/transcribe",
        files={"file": audio_file},
        data={
            "language": "pt",
            "word_timestamps": True,
            "beam_size": 10
        }
    )
    result = response.json()
    
    # Texto completo
    print("Texto:", result["text"])
    
    # Segmentos com timestamps
    for segment in result["segments"]:
        print(f"[{segment['start']:.2f}s -> {segment['end']:.2f}s] {segment['text']}")
```

#### 4. Usando JavaScript/Node.js

```javascript
const FormData = require('form-data');
const fs = require('fs');
const axios = require('axios');

async function transcribeAudio(filePath) {
    const form = new FormData();
    form.append('file', fs.createReadStream(filePath));
    form.append('language', 'pt');
    form.append('word_timestamps', 'true');
    
    try {
        const response = await axios.post('http://localhost:8000/transcribe', form, {
            headers: form.getHeaders()
        });
        
        console.log('Texto:', response.data.text);
        console.log('Idioma:', response.data.language);
        
        return response.data;
    } catch (error) {
        console.error('Erro:', error.response?.data || error.message);
    }
}

// Uso
transcribeAudio('audio.mp3');
```

### ParÃ¢metros da API

| ParÃ¢metro | Tipo | PadrÃ£o | DescriÃ§Ã£o |
|-----------|------|---------|-----------|
| `file` | arquivo | - | Arquivo de Ã¡udio (obrigatÃ³rio) |
| `beam_size` | int | 5 | Tamanho do beam search |
| `language` | string | auto | CÃ³digo do idioma (pt, en, es, etc.) |
| `word_timestamps` | bool | false | Incluir timestamps de palavras |
| `vad_filter` | bool | true | Filtrar silÃªncios com VAD |

### Resposta da API

```json
{
    "text": "Texto transcrito completo",
    "language": "pt",
    "language_probability": 0.95,
    "segments": [
        {
            "start": 0.0,
            "end": 2.5,
            "text": "Primeiro segmento",
            "words": [
                {
                    "start": 0.0,
                    "end": 0.5,
                    "word": "Primeiro",
                    "probability": 0.98
                }
            ]
        }
    ]
}
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

VocÃª pode configurar a API usando variÃ¡veis de ambiente:

```bash
# NÃºmero de threads para CPU
export OMP_NUM_THREADS=4

# Modelo a ser usado (base, small, medium, large)
export WHISPER_MODEL=base

# Porta da aplicaÃ§Ã£o
export PORT=8000
```

### Modelos DisponÃ­veis

| Modelo | Tamanho | Velocidade | Qualidade |
|--------|---------|------------|-----------|
| `tiny` | 39 MB | Muito rÃ¡pido | BÃ¡sica |
| `base` | 74 MB | RÃ¡pido | Boa |
| `small` | 244 MB | MÃ©dio | Muito boa |
| `medium` | 769 MB | Lento | Excelente |
| `large-v3` | 1550 MB | Muito lento | Superior |

## ğŸ³ Docker

### Build da Imagem

```bash
docker build -t faster-whisper-api .
```

### Executar Container

```bash
docker run -p 8000:8000 faster-whisper-api
```

### Docker Compose

```bash
# Iniciar
docker-compose up -d

# Parar
docker-compose down

# Logs
docker-compose logs -f

# Rebuild
docker-compose up --build
```

## ğŸ”§ Desenvolvimento

### Executar em modo desenvolvimento

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Testar a API

```bash
# Health check
curl http://localhost:8000/health

# Testar com arquivo de exemplo
curl -X POST "http://localhost:8000/transcribe" \
     -F "file=@test_audio.wav"
```

## ğŸ“Š Performance

A API estÃ¡ otimizada para CPU com as seguintes configuraÃ§Ãµes:

- **QuantizaÃ§Ã£o INT8**: Reduz uso de memÃ³ria e melhora velocidade
- **VAD Filter**: Remove silÃªncios desnecessÃ¡rios
- **Beam Size**: ConfigurÃ¡vel para balancear velocidade vs qualidade
- **OMP Threads**: Limitado a 4 threads para evitar sobrecarga

### Benchmarks TÃ­picos (CPU)

| Modelo | Arquivo 1min | MemÃ³ria RAM | Tempo |
|--------|--------------|-------------|-------|
| tiny | MP3 | ~500MB | ~5s |
| base | MP3 | ~800MB | ~15s |
| small | MP3 | ~1.2GB | ~45s |

## ğŸš¨ LimitaÃ§Ãµes

- Arquivos muito grandes (>100MB) podem causar timeout
- Uso intensivo de CPU durante transcriÃ§Ã£o
- Primeiro carregamento do modelo pode demorar
- MÃ¡ximo de upload configurado pelo FastAPI

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para detalhes.

## ğŸ”— Links Ãšteis

- [Faster-Whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [FastAPI DocumentaÃ§Ã£o](https://fastapi.tiangolo.com/)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Docker Hub](https://hub.docker.com/)

## ğŸ“ Suporte

Se vocÃª encontrar problemas ou tiver dÃºvidas:

1. Verifique a documentaÃ§Ã£o acima
2. Confira os logs: `docker-compose logs -f`
3. Teste o health check: `curl http://localhost:8000/health`
4. Abra uma issue no repositÃ³rio 